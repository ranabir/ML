{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   },
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   },
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl"
   },
   "outputs": [],
   "source": [
    "def generate_data(m):\n",
    "    # returns the true w as well as X, Y data\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(m):\n",
    "    w = np.random.normal(0, 1, 10)\n",
    "    w /= np.linalg.norm(w)\n",
    "    X = np.random.multivariate_normal(np.zeros(10), np.identity(10), m)\n",
    "    y = np.where(np.dot(X, w) >= 0, 1, 0)\n",
    "    return X, y, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (100, 10)\n",
      "y shape: (100,)\n",
      "w shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "x, y, w = generate_data(100)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"w shape:\", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[ 8.18574463e-01  4.02413624e-01  1.09036209e+00 -7.06579916e-01\n",
      "   1.90886506e+00 -5.34327002e-02  6.33121427e-01 -1.52884168e-01\n",
      "  -5.32060649e-01  1.68893566e-01]\n",
      " [ 1.15411313e-02  5.76832702e-01 -6.48927834e-01  1.42465018e+00\n",
      "  -3.09779779e-01  1.19568367e+00 -8.43687713e-02 -5.83163908e-01\n",
      "  -7.50225430e-01  1.28215799e+00]\n",
      " [-1.08493219e+00  7.93989668e-01 -9.44257337e-01  2.05354061e-01\n",
      "   2.05343767e+00 -4.40837539e-01  3.82697076e-01 -6.45416069e-01\n",
      "  -1.06595389e-01 -9.20936400e-01]\n",
      " [ 3.81375972e-01  1.69843455e-02 -3.45020041e-01 -1.96190221e-01\n",
      "   2.36889137e-01  8.28464272e-01 -1.11768439e+00  2.77006166e+00\n",
      "  -3.51023664e-02  3.54219365e-01]\n",
      " [-1.91388131e-01  8.92121708e-01  2.20551498e+00 -8.66235879e-01\n",
      "  -5.67466499e-01  4.04870227e-01  1.00764333e+00  2.56481493e-01\n",
      "   1.66424076e+00 -6.92609527e-01]\n",
      " [ 2.13388350e-01 -1.41976827e+00  8.77082832e-02 -9.29736564e-02\n",
      "  -5.71617016e-01  6.30871698e-01 -5.10423800e-01 -1.08774285e+00\n",
      "   8.90418733e-01  1.22885008e-01]\n",
      " [ 9.21707930e-01 -5.86633369e-01  1.92360018e+00  8.72134040e-01\n",
      "  -5.90982184e-01 -1.74524080e+00 -9.58870164e-01 -3.43725685e-01\n",
      "  -2.02119367e-01 -1.12431080e+00]\n",
      " [ 7.18552871e-01 -3.68254471e-01  1.17998302e+00  8.19416228e-01\n",
      "   6.25579814e-01  3.16030698e-01 -1.02910063e+00  1.08750173e+00\n",
      "  -2.59693402e+00  5.77501643e-02]\n",
      " [ 2.17014421e+00  1.13412615e+00 -4.12764191e-01 -4.07152691e-01\n",
      "  -1.00770862e+00  1.03834259e+00 -3.85084719e-02  1.24944478e+00\n",
      "   1.09783191e+00 -1.33252138e+00]\n",
      " [ 5.57657484e-01  1.86176567e+00  6.35263633e-01 -9.79361134e-01\n",
      "  -4.97685117e-01  2.19584704e-01 -1.44127554e+00 -4.42161877e-01\n",
      "   4.54452675e-01 -7.44714000e-01]\n",
      " [ 1.66357530e+00 -7.96183332e-02 -5.22106934e-01 -1.32686151e+00\n",
      "   7.62913382e-01 -2.24740616e+00  4.06754664e-01 -5.21228061e-01\n",
      "  -1.25199456e+00 -5.38151334e-01]\n",
      " [ 5.29364204e-01 -9.61258280e-01  8.05138594e-01  4.81988663e-01\n",
      "   3.32974019e-01 -2.98043441e-01 -6.15572099e-01  8.04888650e-01\n",
      "  -1.28538258e+00  8.43266986e-01]\n",
      " [-1.28578312e+00 -9.51310582e-01 -2.91923816e-01  1.82016070e+00\n",
      "  -2.32841075e-01  7.28237440e-01  1.89346355e+00  1.50002467e-01\n",
      "  -9.84090251e-02  1.31511645e-01]\n",
      " [-1.91297060e-01 -3.63776320e-01 -1.01299648e-01 -1.03402769e+00\n",
      "  -1.80481918e-01 -1.38810644e+00 -6.69694266e-01 -1.48152872e+00\n",
      "  -3.73396617e-01  5.16647545e-01]\n",
      " [-8.08525118e-01 -7.57085262e-01 -1.63917508e+00  2.15915889e+00\n",
      "  -8.72522917e-01 -2.96600933e+00 -2.55481170e-01  1.89066066e+00\n",
      "   1.80338327e-01 -9.99134218e-01]\n",
      " [ 5.83879616e-01  9.62783818e-01  1.26489801e+00 -3.92346242e-01\n",
      "   1.76116630e+00  5.35474552e-02 -2.14910648e-01  1.04496331e+00\n",
      "   3.94125503e-01  1.79633508e+00]\n",
      " [-3.51538760e-01  4.99966070e-01 -1.35230077e+00 -5.40298186e-01\n",
      "  -5.46160353e-01 -3.66816748e-01  5.85806805e-01 -1.07526895e+00\n",
      "  -4.06513604e-01 -2.09857676e-01]\n",
      " [ 1.93992839e-02 -1.90666110e-01 -1.41422389e-01 -1.31158170e+00\n",
      "  -3.84142795e-01  1.36040988e+00  6.29388021e-02  2.20054115e+00\n",
      "  -6.62571747e-02 -1.30153376e+00]\n",
      " [-7.83149161e-01 -7.60609040e-01 -9.14710773e-01 -1.17744795e+00\n",
      "   5.89649757e-01 -3.14116839e+00 -3.90591889e-01 -6.19949526e-01\n",
      "   5.42313229e-01  1.98188098e-01]\n",
      " [-8.91329282e-01 -9.17617122e-01 -1.60175345e+00  6.49828509e-01\n",
      "  -5.74413146e-01 -5.41316816e-01  1.38052727e+00 -1.27897761e+00\n",
      "   6.99197918e-01 -7.57198389e-01]\n",
      " [ 3.44690318e-01 -7.34820930e-01 -6.03795677e-01  1.45320175e+00\n",
      "  -1.59666997e+00 -5.79693597e-01  5.28342102e-01 -5.21097309e-02\n",
      "   8.11223654e-01  1.03096668e+00]\n",
      " [ 4.19697453e-01 -1.48368457e+00 -5.07407729e-01 -3.95532184e-01\n",
      "   8.57365840e-01 -7.64887629e-01 -2.49357621e-01  3.62154976e-01\n",
      "   1.89318448e+00  5.26961833e-02]\n",
      " [ 2.08276434e+00 -1.01031014e-02 -8.04822792e-02 -4.95021294e-01\n",
      "   1.61876063e+00  1.08979572e-02 -9.70389798e-01  1.98390764e-01\n",
      "   5.97394083e-01  9.58285143e-02]\n",
      " [ 1.94819952e+00  4.34940184e-01 -6.56297863e-01 -3.82198489e-01\n",
      "  -7.86121413e-01  1.73860185e+00  9.60935595e-02  1.98237003e-01\n",
      "   3.87055156e-01  4.69450494e-01]\n",
      " [-8.49678929e-01  7.18335788e-01  8.89558714e-01 -1.25219754e+00\n",
      "   7.46866373e-01  8.69788414e-01 -1.38600402e+00  1.77027286e-01\n",
      "  -5.72399015e-01 -4.27020418e-01]\n",
      " [ 6.89389060e-03  5.36876752e-01  4.81203005e-02 -1.61100429e+00\n",
      "   4.86591723e-01  1.60414824e+00 -1.98046863e-01  5.11855443e-01\n",
      "  -7.06762839e-03  2.94180855e-01]\n",
      " [ 3.02604686e-01 -7.71258127e-01  8.90750731e-01 -1.87193117e+00\n",
      "  -2.82326045e-01 -2.85760408e-01 -2.16103090e-01  1.74199534e+00\n",
      "  -7.63299943e-01 -2.87513126e-01]\n",
      " [ 4.36296513e-01 -1.20871893e+00 -1.30536529e+00 -6.47662827e-01\n",
      "   7.58349759e-01 -2.27233597e-01  2.06534922e-01  1.19180200e+00\n",
      "  -1.08720889e+00  1.34192298e+00]\n",
      " [ 2.10450041e+00  6.82539097e-01  1.11758166e+00 -5.80687273e-01\n",
      "   6.87316243e-01 -1.10978700e+00  4.06893595e-01 -6.84809589e-01\n",
      "   5.12257935e-01  2.97555940e-01]\n",
      " [-2.70640349e-01 -5.00586478e-01 -9.67059028e-01  2.93375173e-01\n",
      "   1.12975651e+00  1.20351834e-01  7.49176791e-02  9.61425754e-01\n",
      "   6.38016275e-01  1.05115662e+00]\n",
      " [ 8.95606449e-01 -7.33548119e-02 -2.68477843e+00 -1.17858347e+00\n",
      "  -4.63777333e-02  1.11457768e+00 -4.00496590e-02 -5.89221418e-01\n",
      "   1.92511072e-01 -4.31216330e-01]\n",
      " [ 1.15009808e+00  4.49324460e-01  7.78812750e-02  5.57612122e-01\n",
      "   3.37131571e-01 -1.21244960e+00 -5.27488611e-01 -2.50191138e-01\n",
      "  -1.43998302e+00  1.44074484e+00]\n",
      " [ 2.36606624e+00  7.50373306e-01 -7.29380837e-02  3.76478437e-01\n",
      "  -2.47794973e-01  5.13809587e-01  8.17924011e-01  2.17925132e-01\n",
      "   3.75916144e-01 -8.18101088e-01]\n",
      " [ 6.68060575e-01 -1.45350406e+00  4.04625342e-01 -1.13666161e+00\n",
      "   1.45568202e+00 -3.81134949e-01  5.89770940e-01 -7.45413293e-02\n",
      "   3.28686987e-01 -3.93260599e-01]\n",
      " [-3.68957179e-01  1.41643040e+00 -4.28182134e-02 -7.83108818e-01\n",
      "  -1.19855773e-01  2.08587125e+00  1.31789339e-01  1.17077225e+00\n",
      "   1.80916066e+00  1.02488472e+00]\n",
      " [-7.64483567e-01  9.40533752e-01 -1.92101335e-01  1.10322372e+00\n",
      "   5.70946872e-02  1.14728871e+00  3.89314970e-01 -8.97627151e-01\n",
      "  -6.78621212e-01 -1.04676481e+00]\n",
      " [-8.02905111e-01  1.10268359e+00 -5.96355620e-01 -1.10713279e+00\n",
      "  -2.56947028e-01  7.42408610e-01 -1.22799503e+00 -2.90850941e-01\n",
      "  -3.83109366e-03 -6.47477743e-01]\n",
      " [-1.43386935e-01 -1.96463725e-02 -1.45420759e+00 -5.28246415e-01\n",
      "   9.74086557e-01  1.93796481e-01 -2.63176548e-01 -3.58036852e-01\n",
      "  -2.64529221e-01 -1.42467974e-01]\n",
      " [-1.58596724e-01  4.48550562e-01  6.27978949e-01  1.95616372e-01\n",
      "  -3.25716258e-01 -3.40687496e-01  2.05806386e-01  1.08785512e+00\n",
      "  -4.45027846e-01  2.75722266e-01]\n",
      " [-1.70049487e+00 -6.37086238e-01 -1.04064424e+00 -4.08923126e-01\n",
      "   1.23311519e-01 -1.11673880e+00  3.17861288e-02 -9.56377383e-01\n",
      "  -5.96354519e-01 -4.25988482e-01]\n",
      " [ 1.98496650e+00  3.99545727e-01 -3.14942446e-01  6.64521442e-01\n",
      "   7.31771243e-01 -1.51295949e-02 -4.68553987e-02  1.56977746e+00\n",
      "  -1.37553493e-01 -1.81821899e+00]\n",
      " [ 2.27791774e-01  1.50723380e-01 -8.30249229e-01 -1.22540253e+00\n",
      "  -9.51454985e-01 -2.56893637e-01 -8.55533005e-01  1.60995973e+00\n",
      "   6.48700852e-01  1.43971226e+00]\n",
      " [ 2.28124987e-01 -4.07389512e-01 -7.32930782e-01 -1.81537413e+00\n",
      "   1.42997089e-01  1.58703805e-02  2.00493876e-01  4.15427672e-01\n",
      "  -2.13111616e-01 -5.02071956e-01]\n",
      " [-3.58840181e-02 -5.77320933e-01 -7.41651637e-01 -2.91119547e-01\n",
      "   1.04836434e+00 -1.05828530e+00  1.00369634e+00  4.86369231e-01\n",
      "  -1.01092819e-01  4.61050257e-01]\n",
      " [ 1.00998003e+00 -1.49153831e+00  2.01906447e-01  2.29015283e-02\n",
      "   1.19405749e+00 -6.13231544e-01 -2.53964290e-01  5.06245949e-01\n",
      "   4.41511810e-01  4.62218135e-01]\n",
      " [ 5.25809209e-02 -1.39581828e+00 -6.54722838e-01  1.09410764e+00\n",
      "  -1.63184897e+00 -8.80807446e-01 -7.15441574e-01  1.22339751e+00\n",
      "   9.93056398e-01 -2.99836465e-01]\n",
      " [-8.36280197e-02 -4.42163308e-02 -5.98016683e-02  9.34203058e-01\n",
      "  -1.50863607e-01 -1.78082292e-01  1.78087596e+00  1.08268882e+00\n",
      "  -5.81965427e-01 -5.27619653e-02]\n",
      " [-9.34034105e-01  2.62685175e-01 -6.88157685e-01 -1.03654467e+00\n",
      "   2.33397387e+00  1.77573055e+00 -9.53992294e-01 -1.33831732e+00\n",
      "  -1.03959463e+00 -7.45653232e-01]\n",
      " [ 4.30372337e-01  5.01054861e-01 -9.84178507e-01  8.79182518e-02\n",
      "   2.40402869e-01  6.43328180e-01 -7.80987392e-01 -5.14302199e-01\n",
      "  -1.54950507e+00 -1.38586313e+00]\n",
      " [-2.22920123e-01  7.21378137e-01 -8.64048392e-01  5.36779019e-01\n",
      "   2.27692083e-02 -4.01637010e-01 -4.70636061e-03  5.00312303e-01\n",
      "  -2.63829991e-01  2.32057811e-02]\n",
      " [ 1.80190958e+00  2.07724193e-01 -5.48898726e-01  2.45120794e-01\n",
      "  -1.96412993e-02  7.82915620e-02 -4.93214556e-01  3.17536322e-01\n",
      "   1.27963885e+00  7.60957296e-01]\n",
      " [-2.91484314e-01  9.16008176e-02 -6.18841798e-02 -1.61997026e+00\n",
      "  -1.19359765e-01  2.67402966e-01 -8.72955010e-01  1.15006905e+00\n",
      "   6.49923793e-01  7.80170560e-01]\n",
      " [ 6.94539061e-01  7.72406487e-01  6.87170920e-01 -1.11123502e+00\n",
      "  -9.12638519e-01  4.35549466e-02  7.82538988e-01 -1.87249475e-01\n",
      "  -2.68136106e-01 -6.09402487e-01]\n",
      " [-2.03786608e+00 -1.02220454e+00  3.47252817e-01  1.57180319e+00\n",
      "  -1.70003293e-02  1.82324383e+00  9.01226140e-01 -4.29663831e-01\n",
      "   6.67585344e-01  7.68432603e-01]\n",
      " [ 2.14672380e-02 -7.58563199e-01 -2.05796596e+00  6.05056310e-02\n",
      "   1.71865318e+00 -5.16376025e-01  4.16772827e-01 -1.38749843e+00\n",
      "  -9.17836536e-01  3.02343840e-01]\n",
      " [ 1.90364591e+00 -7.53874980e-01 -1.27105452e+00 -5.55449312e-01\n",
      "  -1.31653164e+00  7.71410461e-01 -4.13376537e-01 -1.09807409e+00\n",
      "  -1.44294725e+00  5.78133936e-01]\n",
      " [-8.00118891e-01 -9.16691385e-01  3.06810468e-01  1.60640970e+00\n",
      "   1.57854037e-01 -4.10100173e-01  1.07150296e-01  1.95884445e+00\n",
      "  -3.88813418e-01 -1.32146919e+00]\n",
      " [ 7.99654264e-01 -1.38524342e+00 -9.80437973e-01  9.00264450e-01\n",
      "   5.31527554e-01  6.61083192e-01 -2.13753438e-01  5.70630096e-01\n",
      "   4.76196500e-01 -9.91925505e-01]\n",
      " [-1.68289999e+00  8.54419978e-01  2.64953476e+00 -1.17750497e+00\n",
      "   1.23032053e+00  5.23020060e-02 -1.19763747e+00  6.45451085e-02\n",
      "   1.17182968e+00  3.29490220e-01]\n",
      " [ 5.88470541e-01  9.28072896e-01 -1.38041775e+00 -1.54054733e+00\n",
      "  -9.92491299e-01  1.31336887e+00 -7.08339159e-01 -8.54208688e-01\n",
      "   1.37171492e+00  1.38420523e+00]\n",
      " [-1.51739198e+00 -3.72132120e-01 -9.69449162e-01  2.62429961e-01\n",
      "   2.55285929e+00  7.03104658e-01  4.56209122e-01 -1.95398457e-01\n",
      "  -1.12425789e+00  3.00056709e-01]\n",
      " [-1.94718337e-01 -2.81517329e+00 -9.94379100e-01 -4.26977973e-01\n",
      "   5.98959056e-02  4.95005701e-01  5.65773389e-01  4.21819110e-01\n",
      "  -2.24701289e+00 -1.78288803e-01]\n",
      " [ 1.27136166e+00  1.08061676e-02  6.84580849e-01 -1.65265267e+00\n",
      "   1.02529105e+00 -2.16477729e-01  2.97240478e-01 -6.99633122e-01\n",
      "   9.08129475e-01  1.16566183e-01]\n",
      " [ 1.22994603e+00  1.84149153e-01  1.91402210e-01 -5.62903043e-01\n",
      "  -3.13205579e-01 -2.32504466e-01  2.07838639e-01  3.44481008e-01\n",
      "  -4.24723437e-01 -1.50225918e-01]\n",
      " [-6.21679772e-01  9.74254408e-01 -6.25306805e-01  1.65845402e-02\n",
      "   1.85789125e+00  4.57098700e-02 -1.07238501e+00  1.25489398e+00\n",
      "  -1.65445502e+00 -8.19215438e-01]\n",
      " [-3.29270689e-01  1.25946776e+00 -8.30134021e-01  1.64251304e+00\n",
      "  -1.63406789e+00  8.83912769e-01  6.83823976e-01 -1.07291942e+00\n",
      "  -1.49632462e+00  1.44703253e-01]\n",
      " [ 8.52897870e-01 -1.93405218e-01  7.77719800e-01 -1.00480130e+00\n",
      "   2.91823881e-01 -1.09542073e+00 -3.23642181e-01  6.49748076e-01\n",
      "  -1.29621121e+00 -3.42492798e-01]\n",
      " [-5.97824685e-01  1.27501277e+00  3.45102523e-01 -6.70304096e-01\n",
      "  -3.77005735e-01  7.73388196e-01  1.02425453e-01 -3.61998128e-01\n",
      "   1.24486241e-01 -8.05541281e-01]\n",
      " [ 3.51073334e-01  8.20686674e-01  7.90790413e-01  1.44421502e-01\n",
      "  -5.71208430e-01  5.02183567e-01  3.00215048e-01 -7.20665260e-01\n",
      "  -2.24129838e-01 -3.43506851e-01]\n",
      " [ 1.74254732e+00  7.95509508e-01  4.59647622e-01 -7.92572833e-01\n",
      "  -7.89121089e-01 -6.90411134e-01  5.17690318e-01  3.81089375e-01\n",
      "  -2.27189749e-01 -4.88995476e-01]\n",
      " [ 3.18902718e-01  4.21519484e-02 -1.24954999e+00 -1.47227007e+00\n",
      "   1.09526316e+00 -7.71193644e-01 -6.94123969e-01  2.40917250e-01\n",
      "   1.05574237e+00 -3.83505645e-01]\n",
      " [ 8.48060732e-01  2.72908473e-01 -7.17931082e-03  1.26315998e+00\n",
      "  -1.27235736e-01  9.53987564e-01  8.88137138e-01  3.78178359e-01\n",
      "   9.71129685e-01 -1.73887956e+00]\n",
      " [-2.27900055e-01 -1.02321348e+00 -7.30626866e-01 -1.32804133e+00\n",
      "  -5.21168650e-01 -1.12264926e+00 -1.41299884e+00 -2.72636941e-01\n",
      "  -1.08689404e+00 -3.74430826e-01]\n",
      " [-1.56083521e+00  6.26991659e-01 -2.64526515e-01  9.11886047e-02\n",
      "   3.33715856e-01 -3.38074170e-01  1.46485006e+00 -1.11540927e+00\n",
      "   9.19985857e-01 -3.38021497e-01]\n",
      " [-1.69848284e+00  1.48756760e+00  1.71373875e-01 -2.13899307e-01\n",
      "   2.00268767e+00 -1.16252053e+00  4.65250885e-01  3.09400962e-01\n",
      "   1.27548781e+00 -6.12847729e-01]\n",
      " [-1.35836865e+00 -1.23062751e+00 -4.01719662e-01 -1.48553060e+00\n",
      "  -3.14653784e-01  6.27274530e-01 -1.34409771e+00  8.03250798e-01\n",
      "   6.35053041e-01  1.28859014e+00]\n",
      " [ 9.07239936e-01  6.46706847e-01 -3.38364181e-01 -1.62261030e+00\n",
      "  -1.38886002e+00  5.12227104e-01  1.29622613e+00 -6.37106550e-01\n",
      "   1.87958422e-01 -1.08389555e+00]\n",
      " [-9.45903391e-02  5.81017230e-01  9.49087601e-01  1.05449712e-01\n",
      "  -2.34939790e+00 -2.52045560e-01  3.55348088e-01  1.34114397e+00\n",
      "  -4.75734709e-01  6.95750553e-01]\n",
      " [ 3.96740811e-01  9.10365433e-02  3.70074090e-01 -4.80796849e-02\n",
      "  -1.22990517e+00  5.50784267e-03 -4.06644421e-01  4.68211630e-01\n",
      "  -5.79902072e-01  1.20815906e-01]\n",
      " [-5.48806312e-01  3.67733138e-01  2.68082700e-01  3.11829569e-01\n",
      "  -1.22478955e+00  9.84298108e-01 -8.34934588e-01 -3.17344852e-03\n",
      "   4.77716958e-01  1.52514402e+00]\n",
      " [-2.05203352e-01 -3.46707261e-01  7.42085885e-01  2.56443613e+00\n",
      "  -1.08568680e+00  1.69565549e+00 -6.53950559e-01 -3.28365376e-01\n",
      "  -8.70479651e-02  1.17783400e+00]\n",
      " [-1.04299270e+00  9.63954279e-01  8.11446035e-01 -9.19317749e-02\n",
      "   1.62307903e+00  8.83164335e-01 -3.45560347e-01  2.54618688e-03\n",
      "  -7.99917793e-01  7.09692261e-01]\n",
      " [ 2.04731506e+00  1.74480339e+00 -6.20163075e-01 -4.50181389e-01\n",
      "  -4.28146950e-01  6.40138236e-01  9.12169698e-01 -4.26527303e-01\n",
      "   3.14166415e-01 -9.92004403e-01]\n",
      " [-1.65131069e+00  2.44461416e+00  3.33660576e-02  2.07064985e+00\n",
      "   6.95195097e-01 -6.42321923e-01 -5.65515558e-01 -5.08236061e-01\n",
      "   1.58482580e+00 -1.73579626e-01]\n",
      " [-2.23920744e-01  8.70372167e-01  4.50297257e-01  8.63189105e-01\n",
      "   7.18973117e-01 -8.40955272e-01  6.48665763e-01 -2.93688487e-01\n",
      "  -2.88024080e+00 -1.18610174e+00]\n",
      " [ 2.33741581e+00 -7.05214942e-01  8.22712112e-02  1.16651005e+00\n",
      "   1.60774743e+00  9.03413137e-01 -1.01389051e+00 -2.46683797e-01\n",
      "   7.66862459e-01  1.65893156e+00]\n",
      " [ 4.99427298e-01 -2.19873608e-01  3.32271328e-01 -1.95829379e+00\n",
      "   8.87400809e-01 -3.84825355e-01  1.54348197e+00 -2.80084767e-01\n",
      "  -2.56914842e-01 -2.45104999e-01]\n",
      " [ 1.05813115e+00  1.00922162e+00  1.39973207e+00 -2.67538461e-01\n",
      "  -4.28883906e-02 -8.58734652e-01  1.43229297e-02 -1.34144940e+00\n",
      "   4.67134347e-02 -2.20436830e-01]\n",
      " [-1.49561690e+00 -1.11386313e-01 -1.07606790e+00 -6.67193483e-01\n",
      "  -5.96324040e-01 -9.75290513e-01 -8.00544814e-01 -1.55634552e-01\n",
      "   1.40690853e-01  4.73814511e-01]\n",
      " [ 3.27317296e-01  1.13506270e+00 -6.35801118e-01  8.77345340e-01\n",
      "   4.34286981e-01 -6.63762429e-01  1.64580276e-01 -9.52925736e-02\n",
      "  -1.42097423e+00 -5.17689281e-03]\n",
      " [-2.05256359e+00  3.64378947e-01  5.85140147e-01 -4.95262972e-01\n",
      "  -3.89888843e-01  1.42559599e+00 -1.75086794e-01 -8.45111607e-01\n",
      "  -6.04463948e-02 -6.30089051e-01]\n",
      " [ 1.49230574e+00  4.10589162e-01  7.48327610e-01  7.44725247e-01\n",
      "  -1.23278441e+00 -3.85871599e+00  8.81175535e-01  1.14295954e+00\n",
      "  -8.07195289e-01 -6.96088278e-02]\n",
      " [ 9.39733108e-01  3.78669537e-01  3.49921220e-01 -1.58784750e+00\n",
      "   1.63468093e+00  4.90873540e-01  7.48472870e-01 -6.87890812e-01\n",
      "  -8.64186352e-02 -1.72683259e+00]\n",
      " [ 1.28336457e-01 -9.35371624e-01  6.34703565e-01 -5.52693478e-01\n",
      "  -3.78193886e-01  9.90406715e-04  2.81615534e-01  7.86919329e-01\n",
      "  -5.53022363e-01 -2.78396808e-01]\n",
      " [-1.34375438e+00  1.10673313e-01  1.24724343e+00  4.44836079e-01\n",
      "   1.13381937e+00  3.65059914e-01 -1.01519129e-01  1.24849156e+00\n",
      "  -6.39282501e-01  4.55690675e-01]\n",
      " [ 1.23650344e-01  3.99574043e-02 -5.01142747e-01  9.18733869e-01\n",
      "   1.11645647e+00  1.21025332e+00  8.61311565e-01  3.61161184e-01\n",
      "   5.56739774e-01  5.58531431e-01]\n",
      " [ 3.45369345e-02  8.35812113e-01 -1.09236766e+00  5.15104752e-01\n",
      "   9.29575681e-01  1.43608015e+00 -1.41377279e+00  1.55862464e+00\n",
      "  -9.69675728e-01 -2.01750334e-01]\n",
      " [-3.46789708e-01 -1.29507027e+00 -5.64254438e-01 -1.37745221e+00\n",
      "  -5.98510796e-03  1.30176402e+00 -5.03523144e-03  4.79895179e-01\n",
      "  -5.78091663e-01  2.31092805e-01]\n",
      " [-8.39926487e-01  5.98579068e-02  1.20498898e-02  1.24394185e+00\n",
      "  -9.72732124e-01 -1.50753989e-01  1.29953708e+00 -8.60263463e-01\n",
      "   3.01123203e-01 -8.29647125e-02]\n",
      " [ 9.96222201e-01 -1.18847752e-01 -4.27360393e-01 -1.02379702e+00\n",
      "  -1.70016470e+00  2.53027106e-02  2.49436874e-01 -4.81359540e-01\n",
      "   7.52759554e-02  3.27779885e-01]]\n",
      "y : [0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0\n",
      " 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1]\n",
      "w : [-0.01581773 -0.19645706 -0.56158071  0.15533537 -0.2499374   0.01468718\n",
      " -0.32948004  0.43313601 -0.03739629 -0.51127844]\n"
     ]
    }
   ],
   "source": [
    "x, y, w = generate_data(100)\n",
    "print(\"X :\", x)\n",
    "print(\"y :\", y)\n",
    "print(\"w :\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   },
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(X, y):\n",
    "    logistic_reg = LogisticRegression(max_iter=1000)\n",
    "    logistic_reg.fit(X, y)\n",
    "    return logistic_reg.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   },
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, eta, iterations):\n",
    "    m, n = X.shape\n",
    "    for i in range(iterations):\n",
    "        y_pred = 1 / (1 + np.exp(-np.dot(X, w)))\n",
    "        loss = 0.5 * np.mean((y_pred - y)**2)\n",
    "        grad = np.dot(X.T, y_pred - y) / m\n",
    "        w -= eta * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_square_loss(x, y, w, sigma):\n",
    "    \"\"\"\n",
    "    Gradient of square loss w.r.t w\n",
    "    \"\"\"\n",
    "    y_pred = sigma(np.dot(x, w))\n",
    "    return (y_pred - y) * x\n",
    "\n",
    "def sigma(z):\n",
    "    \"\"\"\n",
    "    Logistic function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent_square_loss(X, y, w_init, eta=0.01, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent with square loss optimization\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    for i in range(max_iter):\n",
    "        index = np.random.randint(X.shape[0])\n",
    "        x, y_ = X[index], y[index]\n",
    "        grad = grad_square_loss(x, y_, w, sigma)\n",
    "        w = w - eta * grad\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, eta, iterations):\n",
    "    m, n = X.shape\n",
    "    for i in range(iterations):\n",
    "        j = np.random.randint(0, m)\n",
    "        x_j = X[j, :].reshape(1, n)\n",
    "        y_j = y[j].reshape(1, 1)\n",
    "        y_pred = 1 / (1 + np.exp(-np.dot(x_j, w)))\n",
    "        loss = 0.5 * (y_pred - y_j)**2\n",
    "        grad = np.dot(x_j.T, y_pred - y_j)\n",
    "        w -= eta * grad\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.73 TiB for an array with shape (50, 100, 150, 200, 250, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cbc7fe2b03b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0malgorithm_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Logistic Regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Gradient Descent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Stochastic Gradient Descent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Mean time taken by {name}: {np.mean(times)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-cbc7fe2b03b0>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(X, y, m, eta, iterations, num_trials)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mw_hat_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-cbc7fe2b03b0>\u001b[0m in \u001b[0;36mgenerate_data\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.73 TiB for an array with shape (50, 100, 150, 200, 250, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_data(m):\n",
    "    w = np.random.normal(0, 1, 10)\n",
    "    w /= np.linalg.norm(w)\n",
    "    X = np.random.multivariate_normal(np.zeros(10), np.identity(10), m)\n",
    "    y = np.where(np.dot(X, w) >= 0, 1, 0)\n",
    "    return X, y, w\n",
    "\n",
    "def logistic_regression(X, y):\n",
    "    logistic_reg = LogisticRegression(max_iter=1000)\n",
    "    logistic_reg.fit(X, y)\n",
    "    return logistic_reg.coef_\n",
    "\n",
    "def gradient_descent_square_loss(X, y, w_init, eta=0.01, max_iter=10):\n",
    "    \"\"\"\n",
    "    Gradient descent with square loss optimization\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    for i in range(max_iter):\n",
    "        index = np.random.randint(X.shape[0])\n",
    "        x, y_ = X[index], y[index]\n",
    "        grad = grad_square_loss(x, y_, w, sigma)\n",
    "        w = w - eta * grad\n",
    "    return w\n",
    "\n",
    "def stochastic_gradient_descent(X, y, eta, iterations):\n",
    "    m, n = X.shape\n",
    "    for i in range(iterations):\n",
    "        j = np.random.randint(0, m)\n",
    "        x_j = X[j, :].reshape(1, n)\n",
    "        y_j = y[j].reshape(1, 1)\n",
    "        y_pred = 1 / (1 + np.exp(-np.dot(x_j, w)))\n",
    "        loss = 0.5 * (y_pred - y_j)**2\n",
    "        grad = np.dot(x_j.T, y_pred - y_j)\n",
    "        w -= eta * grad\n",
    "    return w\n",
    "\n",
    "def evaluate_error(X, y,w, w_hat):\n",
    "    return np.linalg.norm(w - w_hat)**2\n",
    "\n",
    "def run_experiment(X,y, m, eta, iterations, num_trials):\n",
    "    X, y, w = generate_data(m)\n",
    "    start_time = time.time()\n",
    "    w_hat_1 = logistic_regression(X, y)\n",
    "    time_1 = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    w_hat_2 = gradient_descent(X, y, np.zeros(10), eta, iterations)\n",
    "    time_2 = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    w_hat_3 = stochastic_gradient_descent(X, y, np.zeros(10), eta, iterations)\n",
    "    time_3 = time.time() - start_time\n",
    "    error_1 = evaluate_error(X, y, w, w_hat_1)\n",
    "    error_2 = evaluate_error(X, y, w, w_hat_2)\n",
    "    error_3 = evaluate_error(X, y, w, w_hat_3)\n",
    "    return error_1, error_2, error_3, time_1, time_2, time_3\n",
    "\n",
    "m = [50,100,150,200,250]\n",
    "algorithms = [logistic_regression, gradient_descent, stochastic_gradient_descent]\n",
    "algorithm_names = ['Logistic Regression', 'Gradient Descent', 'Stochastic Gradient Descent']\n",
    "for algorithm, name in zip(algorithms, algorithm_names):\n",
    "    errors, times = run_experiment(X, y, m, eta=0.01, iterations=1, num_trials=1)\n",
    "    plt.plot(m_values, errors, label=name)\n",
    "    print(f'Mean time taken by {name}: {np.mean(times)}')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Create a list to store the mean cross-validated accuracy for each depth\n",
    "accuracies = []\n",
    "\n",
    "# Loop over each depth from 1 to 5\n",
    "for depth in range(1, 6):\n",
    "    # Create a decision tree classifier with the specified depth\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=101)\n",
    "    \n",
    "    # Create an AdaBoost classifier with the decision tree as the base learner\n",
    "    ada = AdaBoostClassifier(base_estimator=dt, n_estimators=10, learning_rate=1, random_state=101)\n",
    "    \n",
    "    # Compute the mean cross-validated accuracy\n",
    "    cv_accuracy = np.mean(cross_val_score(ada, X, y, cv=10))\n",
    "    \n",
    "    # Add the accuracy to the list\n",
    "    accuracies.append(cv_accuracy)\n",
    "\n",
    "# Plot the accuracy against base classifier depth\n",
    "plt.plot(range(1, 6), accuracies)\n",
    "plt.xlabel('Base classifier depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
